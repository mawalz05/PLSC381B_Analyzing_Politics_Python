{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a variety of modules\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import tweepy # Must be installed using Anaconda Prompt\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "#from textblob import TextBlob\n",
    "import string\n",
    "#import preprocessor as p\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply for access to a twitter developer account here: https://developer.twitter.com/en/apply-for-access\n",
    "\n",
    "# These are the keys you get with a twitter developer account\n",
    "consumer_key = \"xxxxxxxxxxxxxxxxxxxx\"\n",
    "consumer_secret = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "access_token = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "access_token_secret = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "# Set your authorizaton access in tweepy from your twitter developer account\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/python-in-plain-english/scraping-tweets-with-tweepy-python-59413046e788\n",
    "\n",
    "def scrapetweets(search_words, date_since, numTweets, numRuns):\n",
    "    # Define a for loop to generate tweets at regular intervales\n",
    "    # We cannot make large API calls in one go, so we try T times\n",
    "    #Define a pandas df to store the data\n",
    "    db_tweets = pd.DataFrame(columns = ['username','acctdesc','location','following','followers',\n",
    "                                       'totaltweets','usercreatedts','tweetcreatedts','retweetcount',\n",
    "                                       'text','hashtags'])\n",
    "    program_start = time.time()\n",
    "    for i in range(0, numRuns):\n",
    "        # We will time how long it takes to scrape tweets for each run\n",
    "        start_run = time.time()\n",
    "        \n",
    "        # Collect tweets using the Cursor Object\n",
    "        # .Cursor() returns and object that you can iterate or loop over to acces the data collected.\n",
    "        # Each item in the ieterator has various attributes that you can access for each tweet.\n",
    "        tweets = tweepy.Cursor(api.search, q = search_words, lang = \"en\", since = date_since,\n",
    "                              tweet_mode = 'extended').items(numTweets)\n",
    "        \n",
    "        #Store these tweets in a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "        \n",
    "        # Obtain the following information\n",
    "            # user.screen_name = twitter handle\n",
    "            # user.description = description of the account\n",
    "            # user.location = where he/she is tweeting from\n",
    "            # user.friends_count = number of users this user is following\n",
    "            # user.follower_count = number of users that are following this user\n",
    "            # user.statusts_count = total tweets by user\n",
    "            # user.created_at = When the user account was created\n",
    "            # created_at = when the tweet was created\n",
    "            # retweet_count = number of retweets\n",
    "            # retweeted_status.full_text = full text of the tweet\n",
    "            # tweet.entitiies['hashtags'] = hashtags in the tweet\n",
    "            # tweet.entities['media'] = any media linked in the tweet (photos, videos, gifs, etc)\n",
    "            \n",
    "        # Begin scraping individual tweets\n",
    "        noTweets = 0\n",
    "        \n",
    "        for tweet in tweet_list:\n",
    "            # Pull the values\n",
    "            username = tweet.user.screen_name\n",
    "            acctdesc = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following = tweet.user.friends_count\n",
    "            followers = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            usercreatedts = tweet.user.created_at\n",
    "            tweetcreatedts = tweet.created_at\n",
    "            retweetcount = tweet.retweet_count\n",
    "            hashtags = tweet.entities['hashtags']\n",
    "\n",
    "            try:\n",
    "                text = tweet.retweeted_status.full_text\n",
    "            except AttributeError: #Not a retweet\n",
    "                text = tweet.full_text\n",
    "\n",
    "            #Add the 12 variables to the empty list - ith tweet\n",
    "            ith_tweet = [username, acctdesc, location, following, followers, totaltweets,\n",
    "                        usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
    "            \n",
    "            #Append to dataframe - db_tweets\n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "            \n",
    "            # Increase Counter\n",
    "            noTweets += 1\n",
    "        \n",
    "        #Run Ended\n",
    "        end_run = time.time()\n",
    "        duration_run = round((end_run - start_run)/60, 2)\n",
    "        \n",
    "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "        print('time taken for {} runs to complete is {} mins'.format(i+1, duration_run))\n",
    "        \n",
    "        time.sleep(920) # 15 minute sleep time\n",
    "    \n",
    "    # Once all of the runs have completed, save them to a single csv file\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Obtain timestamp in a readable format\n",
    "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Define path and file name\n",
    "    filename = \"C:\\\\Users\\\\mawal\\\\OneDrive - Binghamton University\\\\Desktop\\\\Desktop_Folders\\\\Teaching\\\\Python\\\\\" + to_csv_timestamp + '_blm_tweets.csv'\n",
    "    \n",
    "    # Store Dataframe in csv with creation date timestamp\n",
    "    db_tweets.to_csv(filename, index = False)\n",
    "    \n",
    "    program_end = time.time()\n",
    "    print('Scraping has completed!')\n",
    "    print('Total time taken to scrape is {} minutes'.format(round((program_end - program_start)/60), 2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of tweets scraped for run 1 is 1000\n",
      "time taken for 1 runs to complete is 1.77 mins\n",
      "no. of tweets scraped for run 2 is 1000\n",
      "time taken for 2 runs to complete is 0.96 mins\n",
      "Scraping has completed!\n",
      "Total time taken to scrape is 33 minutes\n"
     ]
    }
   ],
   "source": [
    "scrapetweets('Black Lives Matter OR BLM', '2020-11-02', 1000, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
